

# =============================================================================
# LiteLLM Configuration for Agentic AI System
# =============================================================================
# Optimized for Mac Studio M3 Ultra with Ollama backend

model_list:
  # Qwen3 235B-A22B - Primary general-purpose model
  - model_name: "qwen3:235b-a22b"
    litellm_params:
      model: "ollama/qwen3:235b-a22b"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "20m"
      supports_function_calling: true
      supports_parallel_function_calling: true
      supports_vision: false
      max_tokens: 32768
      temperature: 0.7
      top_p: 0.9
      gpu_memory_utilization: 0.85
      context_length: 32768

  # mychen76/qwen3_cline_roocode:14b - Code generation and analysis
  - model_name: "mychen76/qwen3_cline_roocode:14b"
    litellm_params:
      model: "ollama/mychen76/qwen3_cline_roocode:14b"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "10m"
      supports_function_calling: true
      supports_parallel_function_calling: false
      supports_vision: false
      max_tokens: 8192
      temperature: 0.1
      top_p: 0.95

  # Qwen3 Embedding 8B - Text embeddings for RAG and semantic search
  - model_name: "qwen3-embedding"
    litellm_params:
      model: "ollama/dengcao/Qwen3-Embedding-8B:Q5_K_M"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "15m"
      supports_function_calling: false
      supports_parallel_function_calling: false
      supports_vision: false
      task_type: "embedding"
      max_tokens: 8192

  # Qwen3 Reranker 8B - Document reranking for improved RAG
  - model_name: "qwen3-reranker"
    litellm_params:
      model: "ollama/dengcao/Qwen3-Reranker-8B:Q5_K_M"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "15m"
      supports_function_calling: false
      supports_parallel_function_calling: false
      supports_vision: false
      task_type: "rerank"

  # Mistral 7B - Fast general-purpose model
  - model_name: "mistral"
    litellm_params:
      model: "ollama/mistral:7b-instruct-q8_0"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "8m"
      supports_function_calling: true
      supports_parallel_function_calling: false
      supports_vision: false
      max_tokens: 8192
      temperature: 0.7
      top_p: 0.9

  # OpenAI GPT-3.5 Turbo - Cloud-based model for comparison and fallback
  - model_name: "gpt-3.5-turbo"
    litellm_params:
      model: "gpt-3.5-turbo"
      api_key: "${OPENAI_API_KEY}"
      api_base: "https://api.openai.com/v1"
      supports_function_calling: true
      supports_parallel_function_calling: true
      supports_vision: false
      max_tokens: 4096
      temperature: 0.7
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0

  # OpenAI GPT-4 - Advanced cloud-based model for complex tasks
  - model_name: "gpt-4"
    litellm_params:
      model: "gpt-4"
      api_key: "${OPENAI_API_KEY}"
      api_base: "https://api.openai.com/v1"
      supports_function_calling: true
      supports_parallel_function_calling: true
      supports_vision: false
      max_tokens: 8192
      temperature: 0.7
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0

# =============================================================================
# GENERAL SETTINGS
# =============================================================================
general_settings:
  # Authentication
  # master_key: "${LITELLM_MASTER_KEY}"

  # Admin settings
  ui_username: "admin"
  ui_password: "${LITELLM_MASTER_KEY}"

  # API settings
  # completion_model: "qwen3:235b-a22b"  # Commented out to allow dynamic model selection

  # Logging
  set_verbose: false
  json_logs: true

  # Cost tracking
  # track_cost_per_model: true

  # Authentication method - using API key authentication
  # custom_auth: "api_key"  # Commented out as this is causing import errors

  # Alerting - temporarily disabled to avoid missing webhook errors
  # alerting: ["slack", "webhook"]
  # alerting_threshold: 300  # seconds

# =============================================================================
# LITELLM SETTINGS
# =============================================================================
litellm_settings:
  # Drop unsupported parameters for Ollama models
  drop_params: true
  # Callbacks for monitoring and logging
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

  # Caching configuration
  cache: true
  cache_params:
    type: "redis"
    host: "agentic-redis"
    port: 6379
    password: null
    ttl: 300  # 5 minutes
    namespace: "litellm_cache"

  # Request/Response processing
  add_function_to_prompt: true

  # Retry configuration
  num_retries: 3
  request_timeout: 600  # 10 minutes for long-running requests

  # Model fallbacks
  fallbacks: [
    {"qwen3:235b-a22b": ["mistral"]},
    {"mychen76/qwen3_cline_roocode:14b": ["qwen3:235b-a22b"]},
    {"qwen3-embedding": ["qwen3:235b-a22b"]},
    {"qwen3-reranker": ["qwen3:235b-a22b"]},
    {"mistral": ["qwen3:235b-a22b"]},
    {"gpt-3.5-turbo": ["qwen3:235b-a22b"]},
    {"gpt-4": ["gpt-3.5-turbo", "qwen3:235b-a22b"]}
  ]

  # Content filtering
  # content_policy: "default"

  # Streaming support
  stream: true

  # Function calling
  supports_function_calling: true

# =============================================================================
# ROUTER SETTINGS
# =============================================================================
router_settings:
  # Load balancing strategy
  routing_strategy: "least-busy"

# =============================================================================
# ROUTER SETTINGS
# =============================================================================
router_settings:
  # Load balancing strategy
  routing_strategy: "least-busy"
  
  # Model aliases for compatibility
  model_group_alias:
    "gpt-3.5-turbo": "gpt-3.5-turbo"
    "gpt-4": "gpt-4"
    # "text-davinci-003": "qwen3:235b-a22b"
    # "code-davinci-002": "mychen76/qwen3_cline_roocode:14b"
    # "text-embedding-ada-002": "qwen3-embedding"
    # "text-embedding-3-small": "qwen3-embedding"
    # "text-embedding-3-large": "qwen3-embedding"
  
  # Retry settings
  retry_after: 0
  allowed_fails: 3
  cooldown_time: 30
  
  # Timeout settings
  timeout: 600  # 10 minutes
  
  # Health check settings
  health_check_interval: 300  # 5 minutes

# =============================================================================
# RATE LIMITING SETTINGS
# =============================================================================
rate_limit_settings:
  # Default rate limits
  default_rate_limit_per_minute: 60
  default_rate_limit_per_hour: 1000
  default_rate_limit_per_day: 10000
  
  # Model-specific rate limits
  model_rate_limits:
    "qwen3:235b-a22b":
      rpm: 60
      tpm: 200000
    "mychen76/qwen3_cline_roocode:14b":
      rpm: 80
      tpm: 100000
    "qwen3-embedding":
      rpm: 200
      tpm: 500000
    "qwen3-reranker":
      rpm: 150
      tpm: 300000
    "mistral":
      rpm: 120
      tpm: 60000
    "gpt-3.5-turbo":
      rpm: 60
      tpm: 90000
    "gpt-4":
      rpm: 20
      tpm: 40000
  
  # User-specific rate limits (can be overridden per API key)
  user_rate_limits:
    "default":
      rpm: 60
      tpm: 30000
    "premium":
      rpm: 200
      tpm: 100000
    "enterprise":
      rpm: 500
      tpm: 250000

# =============================================================================
# AUTHENTICATION AND AUTHORIZATION
# =============================================================================
auth_settings:
  # API key management
  api_key_header_name: "Authorization"
  api_key_prefix: "Bearer "
  
  # User management
  user_api_key_cache_ttl: 3600  # 1 hour
  
  # Team management
  enable_team_management: true
  
  # SSO configuration (if needed)
  # sso_settings:
  #   provider: "oauth2"
  #   client_id: "${OAUTH_CLIENT_ID}"
  #   client_secret: "${OAUTH_CLIENT_SECRET}"
  #   authorization_url: "https://auth.example.com/oauth/authorize"
  #   token_url: "https://auth.example.com/oauth/token"
  #   user_info_url: "https://auth.example.com/oauth/userinfo"

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================
monitoring_settings:
  # Prometheus metrics
  prometheus_metrics: true
  prometheus_port: 4001
  
  # Health checks
  health_check_endpoint: "/health"
  
  # Logging
  log_level: "INFO"
  log_format: "json"
  
  # Tracing (if needed)
  # tracing:
  #   provider: "jaeger"
  #   endpoint: "http://jaeger:14268/api/traces"
  #   service_name: "litellm-proxy"

# =============================================================================
# SECURITY SETTINGS
# =============================================================================
security_settings:
  # CORS configuration
  cors_origins: ["*"]
  cors_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
  cors_headers: ["*"]
  
  # Request validation
  max_request_size: 10485760  # 10MB
  
  # Content filtering
  enable_content_filter: true
  blocked_words: []
  
  # IP filtering (if needed)
  # allowed_ips: ["127.0.0.1", "10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16"]
  # blocked_ips: []

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================
performance_settings:
  # Connection pooling
  max_connections_per_model: 10
  connection_timeout: 30
  
  # Request queuing
  max_queue_size: 1000
  queue_timeout: 300
  
  # Batch processing
  enable_batch_processing: false
  max_batch_size: 10
  batch_timeout: 5
  
  # Memory management - Optimized for Mac Studio M3 Ultra (128GB RAM)
  max_memory_usage: "96GB"
  
  # CPU optimization - M3 Ultra has 24 cores
  worker_processes: 16
  worker_connections: 2000

# =============================================================================
# CUSTOM ENDPOINTS
# =============================================================================
custom_endpoints:
  # Health check endpoint
  - path: "/health"
    method: "GET"
    response: {"status": "healthy", "timestamp": "{{timestamp}}"}
    
  # Model information endpoint
  - path: "/models"
    method: "GET"
    response: {"models": ["qwen3:235b-a22b", "mychen76/qwen3_cline_roocode:14b", "qwen3-embedding", "qwen3-reranker", "mistral", "gpt-3.5-turbo", "gpt-4"]}
    
  # System status endpoint
  - path: "/status"
    method: "GET"
    response: {"system": "agentic-ai", "version": "1.0.0"}

# =============================================================================
# WEBHOOK CONFIGURATION
# =============================================================================
webhook_settings:
  # Success webhooks
  success_webhooks:
    - url: "http://agentic-n8n:5678/webhook/litellm-success"
      headers:
        "Content-Type": "application/json"
        "Authorization": "Bearer ${N8N_WEBHOOK_TOKEN}"
  
  # Failure webhooks
  failure_webhooks:
    - url: "http://agentic-n8n:5678/webhook/litellm-failure"
      headers:
        "Content-Type": "application/json"
        "Authorization": "Bearer ${N8N_WEBHOOK_TOKEN}"

# =============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# =============================================================================
environment_settings:
  development:
    set_verbose: true
    log_level: "DEBUG"
    cache_ttl: 60
    
  production:
    set_verbose: false
    log_level: "INFO"
    cache_ttl: 300
    enable_content_filter: true
    
  testing:
    set_verbose: true
    log_level: "DEBUG"
    cache: false
    rate_limit_settings:
      default_rate_limit_per_minute: 1000

