# LiteLLM Configuration
# This configuration uses template variables that will be replaced by setup scripts

model_list:
  # Primary general-purpose model - {{PRIMARY_MODEL}}
  - model_name: "general"
    litellm_params:
      model: "ollama/{{PRIMARY_MODEL}}"
      api_base: http://host.docker.internal:11434
      temperature: 0.7
      max_tokens: 4096
      stream: true
      tags: ["general", "primary"]
    metadata:
      notes: "Primary general-purpose model for complex reasoning and analysis"
      capabilities: ["reasoning", "analysis", "general_qa"]

  # Code generation and analysis - {{CODE_MODEL}}
  - model_name: "code"
    litellm_params:
      model: "ollama/{{CODE_MODEL}}"
      api_base: http://host.docker.internal:11434
      temperature: 0.5
      max_tokens: 8192
      stream: true
      tags: ["code", "technical"]
    metadata:
      notes: "Specialized model for code generation and technical tasks"
      capabilities: ["code_generation", "code_review", "debugging"]

  # Embedding model - {{EMBEDDING_MODEL}}
  - model_name: "embedding"
    litellm_params:
      model: "ollama/{{EMBEDDING_MODEL}}"
      api_base: http://host.docker.internal:11434
      embedding: true
      tags: ["embedding"]
    metadata:
      notes: "Embedding model for vector operations and semantic search"
      capabilities: ["embeddings", "semantic_search"]

  # Reranking model - {{RERANKING_MODEL}}
  - model_name: "reranking"
    litellm_params:
      model: "ollama/{{RERANKING_MODEL}}"
      api_base: http://host.docker.internal:11434
      tags: ["reranking"]
    metadata:
      notes: "Reranking model for search result optimization"
      capabilities: ["reranking", "relevance_scoring"]

  # Small general-purpose model - {{SMALL_MODEL}}
  - model_name: "fast"
    litellm_params:
      model: "ollama/{{SMALL_MODEL}}"
      api_base: http://host.docker.internal:11434
      temperature: 0.7
      max_tokens: 4096
      stream: true
      tags: ["small", "fast"]
    metadata:
      notes: "Small, fast model for quick tasks and low-latency responses"
      capabilities: ["quick_tasks", "simple_qa", "fast_response"]

  # External fallback models
  - model_name: "external"
    litellm_params:
      model: "openrouter/z-ai/glm-4.5-air:free"
      api_base: "https://openrouter.ai/api/v1"
      api_key: "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"
      temperature: 0.7
      max_tokens: 8192
      tags: ["openrouter", "fallback", "advanced"]
    metadata:
      notes: "GLM 4.5 Air - Free access via OpenRouter API"

  #- model_name: "gpt-3.5-turbo"
  #  litellm_params:
  #    model: "gpt-3.5-turbo"
  #    api_key: "${OPENAI_API_KEY}"
  #    temperature: 0.7
  #    max_tokens: 4096
  #    tags: ["openai", "fallback"]
  #  metadata:
  #    notes: "OpenAI GPT-3.5 Turbo for fallback when local models are unavailable"

  #- model_name: "gpt-4"
  #  litellm_params:
  #    model: "gpt-4"
  #    api_key: "${OPENAI_API_KEY}"
  #    temperature: 0.7
  #    max_tokens: 8192
  #    tags: ["openai", "fallback", "advanced"]
  #  metadata:
  #    notes: "OpenAI GPT-4 for advanced tasks when local models are insufficient"

# Router configuration for intelligent model selection
router_settings:
  routing_strategy: "usage-based-routing"
  
  # Model selection rules
    
  # Fallback chains
  fallbacks: [
    {"general": ["fast"]},
    {"code": ["general"]},
    {"embedding": ["fast"]},
    {"reranking": ["fast"]},
    {"fast": ["general"]},
    {"external": ["general"]}
  ]
  
  # Context-based routing
  context_routing:
    - pattern: "code|function|class|debug|implement"
      preferred_models: ["code", "general"]
    - pattern: "embed|vector|similarity"
      preferred_models: ["embedding"]
    - pattern: "rank|rerank|relevance"
      preferred_models: ["reranking"]
    - pattern: "quick|simple|fast"
      preferred_models: ["fast", "external"]
      
  # Load balancing
  enable_loadbalancing: true
  loadbalancing_strategy: "least-busy"
  
  # Caching
  cache: true
  cache_ttl: 3600
  
  # Retry configuration
  num_retries: 3
  retry_after: 5
  
  # Request timeout
  request_timeout: 600
  
  # Model-specific overrides
  model_config:
    "general":
      max_parallel_requests: 2
      tpm_limit: 100000
    "code":
      max_parallel_requests: 2
      tpm_limit: 150000
    "embedding":
      max_parallel_requests: 10
      batch_size: 100
    "fast":
      max_parallel_requests: 5
      tpm_limit: 200000

# General settings
general_settings:
  # Completion model can be specified via environment or default to primary
  # completion_model: "primary"  # Commented out to allow dynamic model selection
  
  # Master key for authentication
  master_key: "{{LITELLM_MASTER_KEY}}"
  
  # Database for tracking (Set in .env)
  # database_url: "postgresql://litellm:password@postgres:5432/litellm"
  
  # Logging
  log_level: "INFO"
  json_logs: true
  
  # Telemetry
  telemetry: false
  
  # UI Configuration
  ui_access_mode: "admin"
  
  # Health check endpoint
  health_check_interval: 60
  
  # CORS
  allowed_origins: ["*"]
  
  # Rate limiting
  max_budget: 1000
  budget_duration: "1d"
  
  # Request validation
  enforce_user_param: false
  
  # Streaming
  stream: true
  
# Environment-specific settings
environment_variables:
  # Ollama connection
  OLLAMA_HOST: "http://host.docker.internal:11434"
  
  # OpenAI (optional fallback)
  # OPENAI_API_KEY: "${OPENAI_API_KEY}"
  
  # Model loading optimization
  OLLAMA_NUM_PARALLEL: "4"
  OLLAMA_MAX_LOADED_MODELS: "3"
  
  # GPU Memory settings (platform-specific, set by setup scripts)
  # OLLAMA_GPU_OVERHEAD: "2147483648"

# Monitoring and observability
monitoring:
  # Prometheus metrics
  enable_prometheus_metrics: true
  metrics_port: 9090
  
  # Request logging
  log_requests: true
  log_responses: false
  
  # Performance tracking
  track_model_latency: true
  track_token_usage: true

# Security settings
security:
  # API key validation
  enforce_api_key: true
  
  # IP allowlist (commented out for development)
  # allowed_ips: ["127.0.0.1", "172.16.0.0/12"]
  
  # Request size limits
  max_request_size_mb: 10
  
  # Token limits per request
  max_tokens_per_request: 65536

# Mock responses for testing
mock_responses:
  - route: "/v1/models"
    response: {"models": ["general", "code", "embedding", "reranking", "fast"]}
  
  - route: "/health"
    response: {"status": "healthy", "models_loaded": true}
